{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66717fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as pt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import category_encoders as ce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b25957",
   "metadata": {},
   "source": [
    "# Extract_Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c14e087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ordinal categorical feature\n",
    "def to_ordinal(df,column,category):\n",
    "        df[column] = pd.Categorical(df[column], categories = category )\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40aed519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing synonyms of null entries with NaN \n",
    "def replace_to_null(df,lookuptable):\n",
    "    df =df.replace('Data missing or out of range',np.nan)\n",
    "    for column in df :   \n",
    "        if ((column == 'local_authority_highway') | (column == 'lsoa_of_accident_location') | (column =='second_road_class')) :\n",
    "            df[column]=df[column].replace('-1',np.nan)\n",
    "            newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[-1] , 'newvalue' : [np.nan] })\n",
    "            lookuptable=pd.concat([lookuptable,newrow])\n",
    "        elif ((column == 'road_type') | (column == 'second_road_number') |(column == 'weather_conditions')) :\n",
    "            df[column]=df[column].replace(' ',np.nan)\n",
    "            newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[' '] , 'newvalue' : [np.nan]})\n",
    "            lookuptable=pd.concat([lookuptable,newrow])\n",
    "            df[column]=df[column].replace('',np.nan)\n",
    "            newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[''] , 'newvalue' : [np.nan]})\n",
    "            lookuptable=pd.concat([lookuptable,newrow])    \n",
    "    return df,lookuptable        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54e8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing synonyms entries to shorter interpret. \n",
    "def replace_x_to_y(df,column,x,y,lookuptable):\n",
    "        df[column]=df[column].replace(x,y)\n",
    "        newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[x] , 'newvalue' : [y] })\n",
    "        lookuptable=pd.concat([lookuptable,newrow])\n",
    "        return df,lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cdf0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find corresping frequent weather condition on same date and district to impute with\n",
    "def correspondingweather(df,district,date):\n",
    "    if df.loc[(df['local_authority_ons_district']==district)&(df['date']==date),'weather_conditions'].any():\n",
    "        x= df.loc[(df['local_authority_ons_district']==district)&(df['date']==date),'weather_conditions'].value_counts().idxmax()\n",
    "    else :\n",
    "        x=np.nan\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888455ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping a column\n",
    "def drop_column(df,column):\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "#perform CCA\n",
    "def cca(df,column):\n",
    "    df=df.dropna(axis='index', subset=[column])\n",
    "    return df\n",
    "#Handling duplicated rows\n",
    "def duplicates(df,lookuptable):\n",
    "    #Same accidents with same conditions ( 6 duplicate rows ) : remove immidiately\n",
    "    df=df.drop_duplicates(subset=['longitude','latitude','date','time', 'accident_severity' , 'weather_conditions' ,'number_of_casualties', 'number_of_vehicles' ,'pedestrian_crossing_physical_facilities' , 'road_surface_conditions' ,'junction_detail'])\n",
    "    #For number_of_casualties we will consider the the latest as may someone die later or there was who came back to life.\n",
    "    df=df.drop_duplicates(subset=['longitude','latitude','date','time', 'accident_severity' , 'weather_conditions' , 'number_of_vehicles' ,'pedestrian_crossing_physical_facilities' , 'road_surface_conditions' ,'junction_detail'],keep='last')\n",
    "    #For accident_severity  we will consider it when slight as it is the most occurance and this agrees also with the corresponding number_of_vehicles which will be considered as first appearance.\n",
    "    df=df.drop_duplicates(subset=['longitude','latitude','date','time'  ],keep='first')\n",
    "    # for did_police_officer_attend_scene_of_accident we have 2 No category , one with the notice that it is self reported we can compine both in one category : just No\n",
    "    df,lookuptable=replace_x_to_y(df,'did_police_officer_attend_scene_of_accident','No - accident was reported using a self completion  form (self rep only)','No',lookuptable)\n",
    "    return df,lookuptable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cdb06d",
   "metadata": {},
   "source": [
    "## Handling missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7a54d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing(df,lookupTable):\n",
    "    #perform cca\n",
    "    df=cca(df,'longitude')\n",
    "    df=cca(df,'light_conditions')\n",
    "    df=cca(df,'junction_detail')\n",
    "    df=cca(df,'speed_limit')\n",
    "    df=cca(df,'road_surface_conditions')\n",
    "    \n",
    "    #perform frequancy category imputation\n",
    "    maxlocal_authority_highway=df[df['local_authority_highway'].isnull()]['police_force'].value_counts().idxmax()\n",
    "    df,lookupTable=replace_x_to_y(df,'local_authority_highway',np.nan,maxlocal_authority_highway,lookupTable)\n",
    "    maxpedestrian_crossing_human_control ,maxpedestrian_crossing_physical_facilities = df[df['did_police_officer_attend_scene_of_accident']=='No'][['pedestrian_crossing_human_control','pedestrian_crossing_physical_facilities']].value_counts().idxmax()\n",
    "    #replace null pedestrian_crossing_human_control and pedestrian_crossing_physical_facilities of No did_police_officer_attend_scene_of_accident category with most frequent category in the case of No attending\n",
    "    df.loc[((df['did_police_officer_attend_scene_of_accident']=='No')| (df['police_force']=='Lincolnshire')) & (df['pedestrian_crossing_human_control'].isnull()) ,'pedestrian_crossing_human_control']=df.loc[((df['did_police_officer_attend_scene_of_accident']=='No')| (df['police_force']=='Lincolnshire')) & (df['pedestrian_crossing_human_control'].isnull()) ,'pedestrian_crossing_human_control'].fillna(maxpedestrian_crossing_human_control)\n",
    "    df.loc[((df['did_police_officer_attend_scene_of_accident']=='No')| (df['police_force']=='Lincolnshire')) & (df['pedestrian_crossing_physical_facilities'].isnull()) ,'pedestrian_crossing_physical_facilities']=df.loc[((df['did_police_officer_attend_scene_of_accident']=='No')| (df['police_force']=='Lincolnshire')) & (df['pedestrian_crossing_physical_facilities'].isnull()) ,'pedestrian_crossing_physical_facilities'].fillna(maxpedestrian_crossing_physical_facilities)\n",
    "    maxcarriageway_hazards=df.loc[df['did_police_officer_attend_scene_of_accident']==\"No\",'carriageway_hazards'].value_counts().idxmax()\n",
    "    df.loc[(df['did_police_officer_attend_scene_of_accident']=='No') & (df['carriageway_hazards'].isnull()) ,'carriageway_hazards']=df.loc[(df['did_police_officer_attend_scene_of_accident']=='No') & (df['carriageway_hazards'].isnull()) ,'carriageway_hazards'].fillna(maxcarriageway_hazards)\n",
    "    df.loc[((df['first_road_class']=='A')|(df['first_road_class']=='B')|(df['first_road_class']=='Unclassified') | (df['first_road_class']=='C')) & (df['road_type'].isnull()), ['road_type']]=df.loc[((df['first_road_class']=='A')|(df['first_road_class']=='B')|(df['first_road_class']=='Unclassified') | (df['first_road_class']=='C')) & (df['road_type'].isnull()), ['road_type']].replace(np.nan , 'Single carriageway')\n",
    "    df.loc[((df['first_road_class']=='A(M)')|(df['first_road_class']=='Motorway')) & (df['road_type'].isnull()), ['road_type']]=df.loc[((df['first_road_class']=='A(M)')|(df['first_road_class']=='Motorway')) & (df['road_type'].isnull()), ['road_type']].replace(np.nan , 'Dual carriageway')\n",
    "    df.loc[((df['speed_limit']==20)|(df['speed_limit']==30)|(df['speed_limit']==40)) & (df['trunk_road_flag'].isnull()), ['trunk_road_flag']]=df.loc[((df['speed_limit']==20)|(df['speed_limit']==30)|(df['speed_limit']==40)) & (df['trunk_road_flag'].isnull()), ['trunk_road_flag']].replace(np.nan , 'Non-trunk')\n",
    "    df.loc[((df['speed_limit']==50)|(df['speed_limit']==60)|(df['speed_limit']==70)) & (df['trunk_road_flag'].isnull()), ['trunk_road_flag']]=df.loc[((df['speed_limit']==50)|(df['speed_limit']==60)|(df['speed_limit']==70)) & (df['trunk_road_flag'].isnull()), ['trunk_road_flag']].replace(np.nan , 'Trunk (Roads managed by Highways England)')\n",
    "    for index , row in df.iterrows() : \n",
    "        if (df.loc[index,'weather_conditions'])!= (df.loc[index,'weather_conditions']) :\n",
    "            df.loc[index,'weather_conditions'] =  correspondingweather(df,df.loc[index,'local_authority_ons_district'],df.loc[index,'date'])\n",
    "    \n",
    "    # inpute with missing category Arbitrary value \n",
    "    df.loc[(df['junction_detail']=='Not at junction or within 20 metres' ) & (df['junction_control'].isnull()), ['junction_control']]=df.loc[(df['junction_detail']=='Not at junction or within 20 metres' ) & (df['junction_control'].isnull()), ['junction_control']].replace(np.nan , 'Not at junction or within 20 metres')\n",
    "    df.loc[(df['junction_detail']=='Not at junction or within 20 metres' ) & (df['second_road_number'].isnull()), ['second_road_number']]=df.loc[(df['junction_detail']=='Not at junction or within 20 metres' ) & (df['second_road_number'].isnull()), ['second_road_number']].replace(np.nan  , -1)\n",
    "    df.loc[df.index[(df['junction_detail']=='Not at junction or within 20 metres' ) & (df['second_road_class'].isnull())].tolist(), 'second_road_class'] = 'No road'\n",
    "    df.loc[df.index[df['lsoa_of_accident_location'].isnull()].tolist(), 'lsoa_of_accident_location'] = 'missing'\n",
    "    newrow =pd.DataFrame({'feature': ['second_road_class','lsoa_of_accident_location'] , 'oldvalue':[np.nan,np.nan] , 'newvalue' : ['No road','missing']})\n",
    "    lookupTable=pd.concat([lookupTable,newrow])\n",
    "    #apply cca \n",
    "    df=cca(df,'junction_control')\n",
    "    df=cca(df,'second_road_class')\n",
    "    df=cca(df,'second_road_number')\n",
    "    df=cca(df,'weather_conditions')\n",
    "    \n",
    "    #drop redundent column\n",
    "    df=drop_column(df,'local_authority_district')\n",
    "    df=drop_column(df,'local_authority_ons_district')\n",
    "    \n",
    "    #change types\n",
    "    df[['first_road_number','second_road_number']] = df[['first_road_number','second_road_number']].astype(str).astype(float).astype(int)\n",
    "\n",
    "    return df , lookupTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f879eb",
   "metadata": {},
   "source": [
    "## Handeling outliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2be680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For outliers\n",
    "def frequency(column):\n",
    "    label_freq = (column.value_counts() / len(column))*100\n",
    "    return(label_freq)\n",
    "def rare(column, threshold):\n",
    "    label_freq = frequency(column)\n",
    "    i=0\n",
    "    x=[]\n",
    "    while( i < len(label_freq.values) ):\n",
    "        if label_freq.values[i] <= threshold :\n",
    "            x.append(label_freq.index[i])\n",
    "        i= i+1    \n",
    "    return(x)\n",
    "def removeoutliers(df,column,rare):\n",
    "    for index , row in df.iterrows() : \n",
    "        if (df.loc[index,column]) in rare :\n",
    "            df=df.drop(index)\n",
    "    return df   \n",
    "def replacerarecategory(df,column,rare):\n",
    "    for index , row in df.iterrows() : \n",
    "        if (df.loc[index,column]) in rare :\n",
    "            # change \n",
    "            df[column]=df[column].replace(df.loc[index,column],'rare')\n",
    "    return df        \n",
    "def filllookuptable(cleaningdflookupTable, column ,old , new):\n",
    "    newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[old]  , 'newvalue' : [new] })\n",
    "    cleaningdflookupTable=pd.concat([cleaningdflookupTable,newrow])\n",
    "    return cleaningdflookupTable\n",
    "def handle_categoricaloutliers(df,lookupTable):\n",
    "    for column in df :\n",
    "        percentage=5\n",
    "        if ((column=='police_force')|(column=='accident_severity') |(column=='day_of_week')|(column=='local_authority_highway')|(column=='first_road_class')|(column=='second_road_class')|(column=='road_type')|(column=='junction_detail')|(column=='junction_control')|(column=='pedestrian_crossing_human_control')|(column=='pedestrian_crossing_physical_facilities')|(column=='light_conditions')|(column=='weather_conditions')|(column=='road_surface_conditions')|(column=='special_conditions_at_site')|(column=='carriageway_hazards') ):\n",
    "            if (column=='police_force')|(column=='local_authority_highway') :\n",
    "                percentage =0.5\n",
    "            rareofcolumn =rare(df[column],percentage)\n",
    "            for i in rareofcolumn :\n",
    "                lookupTable = filllookuptable(lookupTable,column,i,'rare')\n",
    "            df=replacerarecategory(df,column,rareofcolumn)   \n",
    "    return  df,lookupTable\n",
    "def outliers(df,lookupTable):\n",
    "    cap = df['number_of_vehicles'].quantile(0.99)\n",
    "    df[\"number_of_vehicles\"] = np.where(df[\"number_of_vehicles\"] >cap,cap,df['number_of_vehicles'])\n",
    "    lookupTable=filllookuptable(lookupTable, 'number_of_vehicles' ,'>5' , '5')\n",
    "    minfirst=((df['first_road_number'].value_counts()/len(df['first_road_number']))*100).min()\n",
    "    minsecond=((df['second_road_number'].value_counts()/len(df['second_road_number']))*100).min()\n",
    "    rarefirst_road_number=rare(df.first_road_number, minfirst)\n",
    "    df=removeoutliers(df,'first_road_number',rarefirst_road_number)\n",
    "    raresecond_road_number=rare(df.second_road_number, minsecond)\n",
    "    df=removeoutliers(df,'second_road_number',raresecond_road_number)\n",
    "    # Categorical data\n",
    "    df,lookupTable=handle_categoricaloutliers(df,lookupTable)\n",
    "    return df,lookupTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "398725e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Clean(csv):\n",
    "    #extract csv\n",
    "    df = pd.read_csv(csv , low_memory=False , index_col=0) \n",
    "    # initailize lookup table\n",
    "    cleaningdflookupTable = pd.DataFrame(columns =['feature','oldvalue', 'newvalue'])\n",
    "    # change to ordinal\n",
    "    categories = [\"Slight\", \"Serious\" , \"Fatal\"]\n",
    "    df=to_ordinal(df,'accident_severity',categories)\n",
    "    # sorting df \n",
    "    df = df.sort_values(by= ['date' ,'time','accident_severity'])\n",
    "    df['date']= pd.to_datetime(df['date'], format='%d/%m/%Y')\n",
    "    #drop not useful columns\n",
    "    df=drop_column(df,'accident_reference')\n",
    "    df=drop_column(df,'accident_year')\n",
    "    # change some entries values\n",
    "    df,cleaningdflookupTable=replace_to_null(df,cleaningdflookupTable)\n",
    "    df,cleaningdflookupTable=replace_x_to_y(df,'first_road_number','first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero ',0,cleaningdflookupTable)\n",
    "    df,cleaningdflookupTable=replace_x_to_y(df,'second_road_number','first_road_class is C or Unclassified. These roads do not have official numbers so recorded as zero ',0,cleaningdflookupTable)\n",
    "    # duplicates \n",
    "    df,cleaningdflookupTable = duplicates(df,cleaningdflookupTable)\n",
    "    # missing values\n",
    "    df,cleaningdflookupTable = missing(df,cleaningdflookupTable)\n",
    "    # outliers\n",
    "    df,cleaningdflookupTable = outliers(df,cleaningdflookupTable)\n",
    "    df.to_csv('2016_Accidents_UK_cleaned.csv',index=False)\n",
    "    cleaningdflookupTable.to_csv('cleaningdflookupTable.csv',index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029fd972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaning = Extract_Clean('2016_Accidents_UK.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6295b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Slight     110761\n",
       "Serious     21063\n",
       "rare         1655\n",
       "Name: accident_severity, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaning.accident_severity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88fce67",
   "metadata": {},
   "source": [
    "# Encoding_Load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillDTlookuptable(DTlookuptable,column,old):\n",
    "    i=0\n",
    "    while(i<len(old)):\n",
    "        newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[old[i]] , 'newvalue' : [i] })\n",
    "        DTlookuptable=pd.concat([DTlookuptable,newrow])        \n",
    "        i=i+1\n",
    "    return DTlookuptable\n",
    "\n",
    "def fillDTlookuptable3(DTlookuptable,replace_map_comp):\n",
    "    for i in replace_map_comp.keys():\n",
    "        for j in replace_map_comp[i].keys() :\n",
    "            newrow =pd.DataFrame({'feature': [i] , 'oldvalue':[j] , 'newvalue' : [replace_map_comp[i][j]] })\n",
    "            DTlookuptable=pd.concat([DTlookuptable,newrow])                    \n",
    "    return DTlookuptable\n",
    "\n",
    "def fillDTlookuptable4(DTlookuptable,column,dictionary):\n",
    "    for i in dictionary.keys() :\n",
    "        newrow =pd.DataFrame({'feature': [column] , 'oldvalue':[i] , 'newvalue' : [dictionary[i]] })\n",
    "        DTlookuptable=pd.concat([DTlookuptable,newrow])               \n",
    "    return DTlookuptable\n",
    "\n",
    "def group4(df,column,DTlookuptable):\n",
    "    if (len(frequency(df[column]).values) == len(set(frequency(df[column]).values))) : #perform Count/Frequency Encoding\n",
    "        dictionary=dict(frequency(df[column]))\n",
    "        freqencoding=[]\n",
    "        for i in df[column] :\n",
    "            if i in dictionary.keys():\n",
    "                freqencoding.append(dictionary[i])\n",
    "        df[column] = freqencoding   \n",
    "        DTlookuptable=fillDTlookuptable4(DTlookuptable,column,dictionary)\n",
    "    else : #perform binary Encoding\n",
    "        encoder = ce.BinaryEncoder(cols=[column])\n",
    "        df = encoder.fit_transform(df) \n",
    "    return df , DTlookuptable   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "697788c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df,lookuptable):\n",
    "    # splitting for lsoa_of_accident_location feature :\n",
    "    # Replacing 'missing' to same formate :'N01000000'\n",
    "    df['lsoa_of_accident_location']=df['lsoa_of_accident_location'].replace('missing','M01000000')\n",
    "    lookuptable.loc[lookuptable.feature=='lsoa_of_accident_location','newvalue']=lookuptable.loc[lookuptable.feature=='lsoa_of_accident_location','newvalue'].replace('missing','M01000000')\n",
    "    # splitting by 010\n",
    "    #DTdf.lsoa_of_accident_location= DTdf.lsoa_of_accident_location.str.split(\"010\",expand=True)\n",
    "    df['lsoa_of_accident_location_1'] = df['lsoa_of_accident_location'].astype(str).str[0]\n",
    "    df['lsoa_of_accident_location_2'] = df['lsoa_of_accident_location'].astype(str).str[-5:]\n",
    "    df=df.drop(['lsoa_of_accident_location'], axis=1)\n",
    "    # convert lsoa_of_accident_location_2 type to int \n",
    "    df['lsoa_of_accident_location_2'] = df['lsoa_of_accident_location_2'].astype(str).astype(int)\n",
    "    Class = [\"Unclassified\",\"Motorway\", \"C\" , \"B\" , \"A(M)\" ,\"A\"]\n",
    "    light =[\"Daylight\", \"Darkness - lights lit\" , \"Darkness - no lighting\" ,\"Darkness - lighting unknown\"]\n",
    "    severity = [\"Slight\", \"Serious\" , \"rare\"]\n",
    "    df.first_road_class = pd.Categorical(df.first_road_class, categories = Class )\n",
    "    df.second_road_class = pd.Categorical(df.second_road_class, categories = Class )\n",
    "    df.light_conditions = pd.Categorical(df.light_conditions, categories = light )\n",
    "    df.accident_severity = pd.Categorical(df.accident_severity, categories = severity )\n",
    "    lookuptable=fillDTlookuptable(lookuptable,'first_road_class',Class)\n",
    "    lookuptable=fillDTlookuptable(lookuptable,'second_road_class',Class)\n",
    "    lookuptable=fillDTlookuptable(lookuptable,'light_conditions',light)\n",
    "    lookuptable=fillDTlookuptable(lookuptable,'accident_severity',severity)\n",
    "    for columnname in df:\n",
    "        # Group 1 : label encoding\n",
    "        if ((columnname=='accident_severity') | (columnname=='light_conditions')  | (columnname=='first_road_class')  | (columnname=='second_road_class')) :\n",
    "            df[columnname] = df[columnname].cat.codes\n",
    "            \n",
    "        else :\n",
    "            # Group 2 : Binary encoding\n",
    "            if  ((columnname=='police_force') | (columnname=='local_authority_highway') | (columnname=='lsoa_of_accident_location_1')) :\n",
    "                encoder = ce.BinaryEncoder(cols=[columnname])\n",
    "                df = encoder.fit_transform(df) \n",
    "            #Group 3 : # Replace value encoding \n",
    "            if ((columnname=='urban_or_rural_area') | (columnname=='did_police_officer_attend_scene_of_accident') | (columnname=='trunk_road_flag') ) :\n",
    "                labels = df[columnname].astype('category').cat.categories.tolist()\n",
    "                replace_map_comp = {columnname : {k: v for k,v in zip(labels,list(range(1,len(labels)+1)))}}\n",
    "                lookuptable= fillDTlookuptable3(lookuptable,replace_map_comp)\n",
    "                df.replace(replace_map_comp, inplace=True)\n",
    "            else :\n",
    "            # Group 4 : # Count/Frequency Encoding or Binary encoding\n",
    "                if ((columnname=='day_of_week') | (columnname=='road_type') | (columnname=='junction_detail') | (columnname=='junction_control') | (columnname=='pedestrian_crossing_human_control') | (columnname=='pedestrian_crossing_physical_facilities') | (columnname=='weather_conditions') | (columnname=='road_surface_conditions') | (columnname=='special_conditions_at_site') | (columnname=='carriageway_hazards') ) :\n",
    "                    df,lookuptable=group4(df,columnname,lookuptable)\n",
    "    return df,lookuptable       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49dae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_float(time):\n",
    "    hours, minutes = time.split(':')\n",
    "    total_seconds = int(hours) * 3600 + int(minutes) * 60\n",
    "    return float(total_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4194dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoding_Load(csv,lookuptable):\n",
    "    # extract csv\n",
    "    df = pd.read_csv(csv, index_col=0) \n",
    "    # initailize lookup table\n",
    "    EncodinglookupTable = pd.read_csv(lookuptable) \n",
    "    # Discretization\n",
    "    df['date']= pd.to_datetime(df['date'])\n",
    "    df['week_number']=df['date'].dt.isocalendar().week\n",
    "    # Drop date column \n",
    "    df = df.drop(['date'], axis=1)\n",
    "    #converting 'time' column to be float \n",
    "    df['time'] = df['time'].apply(time_to_float)\n",
    "    # Encoding\n",
    "    df,EncodinglookupTable=encoding(df,EncodinglookupTable)\n",
    "    df.to_csv('2016_Accidents_UK_cleaned_encoded.csv',index=False)\n",
    "    EncodinglookupTable.to_csv('EncodinglookupTable.csv',index=False)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e7d5068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = Encoding_Load('2016_Accidents_UK_cleaned.csv','cleaningdflookupTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "31a92627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_X = df.drop(['accident_severity'], axis=1)\n",
    "ML_Y = df['accident_severity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534ec7f",
   "metadata": {},
   "source": [
    "# Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1dd674b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(ML_X)\n",
    "# Normalize the entire dataset\n",
    "ML_X_norm = scaler.transform(ML_X)\n",
    "ML_X = pd.DataFrame(ML_X_norm, columns=ML_X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57869bb6",
   "metadata": {},
   "source": [
    "# Features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "890e79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureselection(df,target,threshold):\n",
    "    corr_matrix = df.corr()\n",
    "    target_var = target\n",
    "    corr_with_target = corr_matrix[target_var].abs().sort_values(ascending=False)\n",
    "    corr_threshold = threshold\n",
    "    selected_columns = list(corr_with_target[( corr_with_target > corr_threshold) & (corr_with_target < 1)].index)\n",
    "    ML_X = df[selected_columns].reset_index()\n",
    "    return ML_X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0e71f",
   "metadata": {},
   "source": [
    "# Splitting ( 60% Training set - 20% Validation set - 20% Test set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffb2af",
   "metadata": {},
   "source": [
    "## Random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "defbd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(ML_X,ML_Y):\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test_random, y_train, y_test_random = train_test_split(ML_X,ML_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Split training set into training and validation sets\n",
    "    X_train_random, X_val_random, y_train_random, y_val_random = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    return X_train_random , y_train_random , X_val_random , y_val_random , X_test_random , y_test_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359e7a1",
   "metadata": {},
   "source": [
    "## Stratified K-Fold sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "936c3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stf_sampling(ML_X,ML_Y):\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test_stf, y_train, y_test_stf = train_test_split(ML_X, ML_Y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Define the number of folds\n",
    "    splits = 5\n",
    "\n",
    "    # StratifiedKFold cross-validation method\n",
    "    stf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "    return X_train , y_train ,  X_test_stf , y_test_stf , stf\n",
    "def Stf(stf,model , X_train, y_train):\n",
    "    # Iterate over the folds and split the training set into training and validation sets\n",
    "    accuracy=[]\n",
    "    #precision=[]\n",
    "    #recall=[]\n",
    "    for train_index, val_index in stf.split(X_train, y_train):\n",
    "        X_train_stf, X_val_stf = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_stf, y_val_stf = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        # Train and evaluate the model \n",
    "        model.fit(X_train_stf, y_train_stf)\n",
    "        y_pred = model.predict(X_val_stf)\n",
    "        accuracy.append(accuracy_score(y_val_stf, y_pred))\n",
    "        #precision.append(precision_score(y_val_stf, y_pred))\n",
    "        #recall.append(recall_score(y_val_stf, y_pred))\n",
    "    avg_accuracy = np.mean(accuracy)\n",
    "#     avg_precision = np.mean(precision)\n",
    "#     avg_recall = np.mean(recall)\n",
    "    return avg_accuracy , model  # ,avg_precision,avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d68ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Finally, evaluate your model on the testing set using the best hyperparameters found during cross-validation\n",
    "# model.fit(X_train_full, y_train_full)\n",
    "# test_score = model.score(X_test, y_test)\n",
    "# print(\"Test score: {}\".format(test_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74749c70",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74490455",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a KNN model with random/Stratified K-Fold sampling with feature selection\n",
    "x = featureselection(df,'accident_severity',0.018)\n",
    "n_neighbors_values = [9 , 20 , 50 ,80 , 150]\n",
    "# first is random sampling\n",
    "KNN_X_train_random , KNN_y_train_random , KNN_X_val_random , KNN_y_val_random , KNN_X_test_random , KNN_y_test_random = random_sampling(x,ML_Y)\n",
    "# loop over the hyperparameters and fit a KNN model for each value\n",
    "KNN_random_best_accuracy = 0\n",
    "KNN_random_best_n_neighbors = None\n",
    "best_knn_random = None\n",
    "for n in n_neighbors_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)\n",
    "    knn.fit(KNN_X_train_random, KNN_y_train_random)\n",
    "    y_pred = knn.predict(KNN_X_val_random)\n",
    "    accuracy = accuracy_score(KNN_y_val_random, y_pred)\n",
    "    if accuracy > KNN_random_best_accuracy:\n",
    "        KNN_random_best_accuracy = accuracy\n",
    "        KNN_random_best_n_neighbors = n\n",
    "        best_knn_random = knn\n",
    "\n",
    "# evaluate the performance of the KNN model on the test set\n",
    "y_pred = best_knn_random.predict(KNN_X_test_random)\n",
    "accuracy_score_KNN_random = accuracy_score(KNN_y_test_random, y_pred)\n",
    "precision_score_KNN_random = precision_score(KNN_y_test_random, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_KNN_random = recall_score(KNN_y_test_random, y_pred , average='weighted')\n",
    "cm = confusion_matrix(KNN_y_test_random, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3cfcaf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.830011986814504, 0.793857468955879, 0.830011986814504, 50)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_KNN_random , precision_score_KNN_random , recall_score_KNN_random , KNN_random_best_n_neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8272e5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22156,     0,     0],\n",
       "       [ 4202,     2,     0],\n",
       "       [  335,     1,     0]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d5d5fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second is stf sampling\n",
    "KNN_stf_best_accuracy = 0\n",
    "KNN_stf_best_n_neighbors = None\n",
    "best_knn_stf = None\n",
    "KNN_X_train_stf , KNN_y_train_stf ,  KNN_X_test_stf , KNN_y_test_stf , stf= stf_sampling(x,ML_Y)\n",
    "n_neighbors_values = [9 , 20 , 30]\n",
    "for n in n_neighbors_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=n)    \n",
    "    avg_accuracy = Stf(stf,knn, KNN_X_train_stf, KNN_y_train_stf)\n",
    "    if avg_accuracy > KNN_stf_best_accuracy:\n",
    "        KNN_stf_best_accuracy = avg_accuracy\n",
    "        KNN_stf_best_n_neighbors = n\n",
    "        best_knn_stf = knn\n",
    "\n",
    "# evaluate the performance of the KNN model on the test set\n",
    "y_pred = best_knn_stf.predict(KNN_X_test_stf)\n",
    "accuracy_score_KNN_stf = accuracy_score(KNN_y_test_stf, y_pred)\n",
    "precision_score_KNN_stf = precision_score(KNN_y_test_stf, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_KNN_stf = recall_score(KNN_y_test_stf, y_pred , average='weighted')\n",
    "confusion_matrix_KNN_stf = confusion_matrix(KNN_y_test_random, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1630a083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8298996104285287, 0.7575153794909943, 0.8298996104285287, 30)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_KNN_stf , precision_score_KNN_stf , recall_score_KNN_stf , KNN_stf_best_n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be92d82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22145,    11,     0],\n",
       "       [ 4194,    10,     0],\n",
       "       [  334,     2,     0]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_KNN_stf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c9435",
   "metadata": {},
   "source": [
    "### KNN is quite fast classification technique but it faied to predict class 'rare' for any test point but we can see in stf sampling the KNN performed better in pridection even though it is still iniffeccient "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3d334",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ca52d76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a logistic regression model with L2 regularization applying different c strenght values and random/Stratified K-Fold sampling\n",
    "c=[0.001 , 0.01 ,0.1]\n",
    "# Fisrt Try Random sampling \n",
    "X_train_random , y_train_random , X_val_random , y_val_random , X_test_random , y_test_random = random_sampling(ML_X,ML_Y)\n",
    "models_random = []\n",
    "for C in c:\n",
    "    model = LogisticRegression(solver='sag', C=C, max_iter=10000)\n",
    "    # fit the model on the training set\n",
    "    model.fit(X_train_random, y_train_random)\n",
    "    models_random.append(model)\n",
    "    \n",
    "# evaluate the performance of each model on the validation set\n",
    "validation_scores_random = []\n",
    "for model in models_random:\n",
    "    score = model.score(X_val_random, y_val_random)\n",
    "    validation_scores_random.append(score)\n",
    "\n",
    "# select the best hyperparameter based on the validation score\n",
    "best_index = validation_scores_random.index(max(validation_scores_random))\n",
    "best_model = models_random[best_index]\n",
    "best_c = c[best_index]\n",
    "y_pred = best_model.predict(X_test_random)\n",
    "# evaluate the best model on the test set\n",
    "accuracy_score_random = accuracy_score(y_test_random, y_pred)\n",
    "precision_score_random = precision_score(y_test_random, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_random = recall_score(y_test_random, y_pred , average='weighted')\n",
    "confusion_matrix_LR_random = confusion_matrix(y_test_random, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "770a8288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8299745280191789,\n",
       " 0.773919656904943,\n",
       " 0.8299745280191789,\n",
       " array([[22154,     2,     0],\n",
       "        [ 4201,     2,     1],\n",
       "        [  335,     0,     1]], dtype=int64),\n",
       " 0.01)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_random , precision_score_random , recall_score_random , confusion_matrix_LR_random , best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01de21ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second Try stf sampling \n",
    "LR_X_train_stf , LR_y_train_stf ,  LR_X_test_stf , LR_y_test_stf , stf= stf_sampling(x,ML_Y)\n",
    "\n",
    "models_stf = []\n",
    "validation_accuracy = []\n",
    "\n",
    "for C in c:\n",
    "    model = LogisticRegression(solver='sag', C=C, max_iter=10000)\n",
    "    # fit the model on the training set\n",
    "    avg_accuracy = Stf(stf , model, LR_X_train_stf , LR_y_train_stf)\n",
    "    models_stf.append(model)\n",
    "    validation_accuracy.append(avg_accuracy)\n",
    "\n",
    "# select the best hyperparameter based on the validation score\n",
    "LR_stf_best_index = validation_accuracy.index(max(validation_accuracy))\n",
    "best_LR_stf = models_stf[best_index]\n",
    "best_C = c[best_index]\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "y_pred = best_LR_stf.predict(LR_X_test_stf)\n",
    "# evaluate the best model on the test set\n",
    "accuracy_score_stf = accuracy_score(y_test_stf, y_pred)\n",
    "precision_score_stf = precision_score(y_test_stf, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_stf = recall_score(y_test_stf, y_pred , average='weighted' , zero_division=0)\n",
    "confusion_matrix_LR_stf = confusion_matrix(y_test_random, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b32e7050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8299370692238538, 0.6887955388718798, 0.8299370692238538)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_stf , precision_score_stf , recall_score_stf ,confusion_matrix_LR_stf ,  best_C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d11bb",
   "metadata": {},
   "source": [
    "#  Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b28de061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a Naive Bayes model with L2 regularization applying different alpha strenght values and random/Stratified K-Fold sampling\n",
    "alpha_values=[0.00001 , 0.0001 , 0.001 , 0.01]\n",
    "# Fisrt Try Random sampling \n",
    "NB_X_train_random , NB_y_train_random , NB_X_val_random , NB_y_val_random , NB_X_test_random , NB_y_test_random = random_sampling(ML_X,ML_Y)\n",
    "scaler = MinMaxScaler()\n",
    "NB_X_train_random = scaler.fit_transform(NB_X_train_random)\n",
    "NB_X_val_random = scaler.transform(NB_X_val_random)\n",
    "NB_X_test_random = scaler.transform(NB_X_test_random)\n",
    "NB_models_random = []\n",
    "NB_validation_scores_random = []\n",
    "for a in alpha_values :\n",
    "    nb = MultinomialNB(alpha=a)\n",
    "    nb.fit(NB_X_train_random, NB_y_train_random)\n",
    "    NB_models_random.append(nb)\n",
    "    score = nb.score(NB_X_val_random, NB_y_val_random)\n",
    "    NB_validation_scores_random.append(score)\n",
    "    \n",
    "# select the best alpha based on the validation score\n",
    "NB_random_best_index = NB_validation_scores_random.index(max(NB_validation_scores_random))\n",
    "NB_random_best_model = NB_models_random[NB_random_best_index]\n",
    "NB_random_best_alpha = alpha_values[NB_random_best_index]\n",
    "y_pred = NB_random_best_model.predict(NB_X_test_random)\n",
    "\n",
    "# evaluate the best model on the test set\n",
    "accuracy_score_NB_random = accuracy_score(NB_y_test_random, y_pred)\n",
    "precision_score_NB_random = precision_score(NB_y_test_random, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_NB_random = recall_score(NB_y_test_random, y_pred , average='weighted')    \n",
    "confusion_matrix_NB_random = confusion_matrix(NB_y_test_random, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a59b21b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8299370692238538,\n",
       " 0.6887955388718798,\n",
       " 0.8299370692238538,\n",
       " 1e-05,\n",
       " array([[22156,     0,     0],\n",
       "        [ 4204,     0,     0],\n",
       "        [  336,     0,     0]], dtype=int64))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_NB_random , precision_score_NB_random , recall_score_NB_random , NB_random_best_alpha , confusion_matrix_NB_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3bf4fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second Try stf sampling\n",
    "NB_stf_best_accuracy = 0\n",
    "best_NB_stf = None\n",
    "NB_stf_best_alpha = None\n",
    "NB_X_train_stf , NB_y_train_stf ,  NB_X_test_stf , NB_y_test_stf , stf= stf_sampling(ML_X,ML_Y)\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "NB_X_train_stf = scaler.fit_transform(NB_X_train_stf)\n",
    "NB_X_train_stf = pd.DataFrame(NB_X_train_stf, columns=ML_X.columns)\n",
    "NB_X_test_stf = scaler.transform(NB_X_test_stf) \n",
    "NB_X_test_stf = pd.DataFrame(NB_X_test_stf, columns=ML_X.columns)\n",
    "\n",
    "for a in alpha_values:\n",
    "    nb = GaussianNB(var_smoothing=a)\n",
    "    avg_accuracy, nb = Stf(stf,nb, NB_X_train_stf, NB_y_train_stf)  \n",
    "    if avg_accuracy > NB_stf_best_accuracy:\n",
    "        NB_stf_best_accuracy = avg_accuracy\n",
    "        best_NB_stf = nb\n",
    "        NB_stf_best_alpha = a\n",
    "\n",
    "\n",
    "# evaluate the performance of the KNN model on the test set\n",
    "y_pred = best_NB_stf.predict(NB_X_test_stf)\n",
    "accuracy_score_NB_stf = accuracy_score(NB_y_test_stf, y_pred)\n",
    "precision_score_NB_stf = precision_score(NB_y_test_stf, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_NB_stf = recall_score(NB_y_test_stf, y_pred , average='weighted')\n",
    "confusion_matrix_NB_stf = confusion_matrix(NB_y_test_stf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c292b920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6777045250224752, 0.7411663538297, 0.6777045250224752, 0.01)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " accuracy_score_NB_stf , precision_score_NB_stf ,  recall_score_NB_stf , NB_stf_best_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2648ce18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17765,   483,  3908],\n",
       "       [ 3009,   145,  1050],\n",
       "       [  148,     6,   182]], dtype=int64)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix_NB_stf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af22a3b3",
   "metadata": {},
   "source": [
    "## stf Naive Bayes  has proved a better performance when regularization parameter is high 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc74b82",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1fd319c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy:  0.7875972254791828\n",
      "Test accuracy:  0.7891069823194486\n"
     ]
    }
   ],
   "source": [
    "# apply stf sampling and Create a neural network classifier with initializain of hypermater \n",
    "NN_X_train_stf , NN_y_train_stf ,  NN_X_test_stf , NN_y_test_stf , stf= stf_sampling(ML_X,ML_Y)\n",
    "NN_stf_best_accuracy = 0\n",
    "best_NN_stf = None\n",
    "NN_stf_best_activation = None\n",
    "NN_stf_best_reg = None\n",
    "NN_stf_best_learning_rate = None\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', alpha=0.0001, max_iter=500, random_state=42 , learning_rate_init = 0.001)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "avg_accuracy , clf = Stf(stf,clf, NB_X_train_stf, NB_y_train_stf) \n",
    "#val_loss = clf.score(X_val, y_val)\n",
    "\n",
    "# Evaluate the classifier on the testing set\n",
    "y_pred = clf.predict(NN_X_test_stf)\n",
    "test_accuracy = accuracy_score(NN_y_test_stf, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Validation accuracy: \", avg_accuracy)\n",
    "print(\"Test accuracy: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "37143d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# when learning rate is decreased to be 0.0001\n",
    "X_train, X_test_stf, y_train, y_test_stf = train_test_split(ML_X, ML_Y, test_size=0.2, random_state=42)\n",
    "# Define the number of folds\n",
    "splits = 5\n",
    "# StratifiedKFold cross-validation method\n",
    "stf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "accuracy=[]\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', alpha=0.0001, max_iter=300, random_state=42 , learning_rate_init = 0.0001)\n",
    "for train_index, val_index in stf.split(X_train, y_train):\n",
    "        X_train_stf, X_val_stf = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_stf, y_val_stf = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        # Train and evaluate the model \n",
    "        clf.fit(X_train_stf, y_train_stf)\n",
    "        y_pred = model.predict(X_val_stf)\n",
    "        accuracy.append(accuracy_score(y_val_stf, y_pred))\n",
    "avg_accuracy = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3aa940ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8297856409991692, 0.824030717362802)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "046e74f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the performance of the KNN model on the test set\n",
    "y_pred = clf.predict(X_test_stf)\n",
    "accuracy_score_NN_stf = accuracy_score(y_test_stf, y_pred)\n",
    "precision_score_NN_stf = precision_score(y_test_stf, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_NN_stf = recall_score(y_test_stf, y_pred , average='weighted')\n",
    "confusion_matrix_NN_stf = confusion_matrix(y_test_stf, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ce2b8469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8264534012586155,\n",
       " 0.7537664275773187,\n",
       " 0.8264534012586155,\n",
       " array([[21907,   247,     2],\n",
       "        [ 4050,   152,     2],\n",
       "        [  295,    37,     4]], dtype=int64))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score_NN_stf , precision_score_NN_stf , recall_score_NN_stf , confusion_matrix_NN_stf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "adc8f251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#  max_iter= 300\n",
    "# when learning rate is decreased to be 0.0001\n",
    "X_train, X_test_stf, y_train, y_test_stf = train_test_split(ML_X, ML_Y, test_size=0.2, random_state=42)\n",
    "# Define the number of folds\n",
    "splits = 5\n",
    "# StratifiedKFold cross-validation method\n",
    "stf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n",
    "accuracy=[]\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, 50), activation='relu', alpha=0.0001, max_iter=300, random_state=42 , learning_rate_init = 0.0001)\n",
    "for train_index, val_index in stf.split(X_train, y_train):\n",
    "        X_train_stf, X_val_stf = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_train_stf, y_val_stf = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        # Train and evaluate the model \n",
    "        clf.fit(X_train_stf, y_train_stf)\n",
    "        y_pred = model.predict(X_val_stf)\n",
    "        accuracy.append(accuracy_score(y_val_stf, y_pred))\n",
    "avg_accuracy = np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62bd28c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8161147737488762,\n",
       " 0.7432218928913371,\n",
       " 0.8161147737488762,\n",
       " array([[21499,   649,     8],\n",
       "        [ 3916,   283,     5],\n",
       "        [  279,    52,     5]], dtype=int64))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the performance of the KNN model on the test set\n",
    "y_pred = clf.predict(X_test_stf)\n",
    "accuracy_score_NN_stf = accuracy_score(y_test_stf, y_pred)\n",
    "precision_score_NN_stf = precision_score(y_test_stf, y_pred ,average='weighted' , zero_division=0)\n",
    "recall_score_NN_stf = recall_score(y_test_stf, y_pred , average='weighted')\n",
    "confusion_matrix_NN_stf = confusion_matrix(y_test_stf, y_pred)\n",
    "accuracy_score_NN_stf , precision_score_NN_stf , recall_score_NN_stf , confusion_matrix_NN_stf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "96518930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we achieved more validation average accuracy  when decreasing the learning rate to be 0.0001 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b25872",
   "metadata": {},
   "source": [
    "## Neural Network has achieved best classification performance due to  accuracy_score, precision_score, recall_score, and confusion_matrix cobinations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
